{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. bs이용하여 웹 페이지 내용 얻기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching pages: 100%|##########| 3/3 [00:00<00:00,  5.89it/s]\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "loader = WebBaseLoader([\"https://lilianweng.github.io/posts/2023-06-23-agent/\",\"https://lilianweng.github.io/posts/2023-03-15-prompt-engineering/\",\"https://lilianweng.github.io/posts/2023-10-25-adv-attack-llm/\"])\n",
    "loader.requests_per_second = 1\n",
    "docs = loader.aload()\n",
    "texts = []\n",
    "for data in docs:\n",
    "    texts.append(data.page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. splitter 이용하여 웹 페이지 내용 분할"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "page_content=\"LLM Powered Autonomous Agents | Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nLil'Log\"\n",
      "page_content=\"Lil'Log\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\n\\nPosts\\n\\n\\n\\n\\nArchive\\n\\n\\n\\n\\nSearch\"\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=100,\n",
    "    chunk_overlap=50,\n",
    "    length_function=len,\n",
    "    is_separator_regex=False,\n",
    ")\n",
    "texts = text_splitter.create_documents(texts)\n",
    "print(texts[0])\n",
    "print(texts[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. vector 스토어에 임베딩하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/kakao/ai-advanced/rag/lib/python3.11/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Long-term memory: This provides the agent with the capability to retain and recall (infinite)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "with open(\"../api_key.txt\", 'r') as file:\n",
    "    # 파일 전체 내용을 읽어오기\n",
    "    api_key = file.read()\n",
    "\n",
    "embeddings_model = OpenAIEmbeddings(openai_api_key=api_key)\n",
    "\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_community.embeddings.sentence_transformer import (\n",
    "    SentenceTransformerEmbeddings,\n",
    ")\n",
    "\n",
    "# create the open-source embedding function\n",
    "embedding_function = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# load it into Chroma\n",
    "db = Chroma.from_documents(texts, embedding_function)\n",
    "\n",
    "# query it\n",
    "query = \"agent memory\"\n",
    "docs = db.similarity_search(query)\n",
    "\n",
    "# print results\n",
    "print(docs[0].page_content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 유사도 구하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Relevance Score**: 0.7\n",
      "\n",
      "**Explanation**: The retrieved chunk discusses long-term memory, which is related to memory, one of the key terms in the user query. While the chunk doesn't directly mention \"agent memory,\" it does cover the concept of memory retention and recall, which aligns with the user query's topic. However, the repetition of the same sentence multiple times may reduce the overall relevance score. \n",
      "\n",
      "[relevance: yes]\n"
     ]
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder, PromptTemplate\n",
    "\n",
    "retriever = db.as_retriever()\n",
    "model = ChatOpenAI(model=\"gpt-3.5-turbo-0125\",openai_api_key=api_key)\n",
    "template = \"\"\"\n",
    "        You are an AI system that evaluates the relevance between a user query and a retrieved chunk of text. Your task is to determine if the retrieved chunk is relevant to the user's query based on the following criteria:\n",
    "\n",
    "        1. **Keyword Matching**: Check if the key terms in the user query appear in the retrieved chunk.\n",
    "        2. **Semantic Similarity**: Assess whether the meaning of the retrieved chunk aligns with the intent of the user query, even if exact keywords are not present.\n",
    "        3. **Context Appropriateness**: Ensure the context of the retrieved chunk makes sense in relation to the user's query.\n",
    "\n",
    "        Provide a relevance score between 0 and 1, where 0 means completely irrelevant and 1 means highly relevant. Additionally, provide a brief explanation for your score.\n",
    "        If the Relevance Score is more than 0.5, [relevance: yes] is output, and if it is less than 0.5, [relevance: no] is output.\n",
    "        \n",
    "        ### Example:\n",
    "\n",
    "        **User Query**: \"What are the health benefits of green tea?\"\n",
    "\n",
    "        **Retrieved Chunk**: \"Green tea is known for its high antioxidant content, which can help reduce inflammation and improve heart health.\"\n",
    "\n",
    "        **Relevance Score**: 1.0\n",
    "\n",
    "        **Explanation**: The retrieved chunk directly addresses the health benefits of green tea by mentioning its antioxidant content and positive effects on inflammation and heart health.\n",
    "\n",
    "        Now, evaluate the following pairs:\n",
    "\n",
    "        **User Query**: {user_query}\n",
    "\n",
    "        **Retrieved Chunk**: {context}\n",
    "\n",
    "        **Relevance Score**:\n",
    "\n",
    "        **Explanation**:\n",
    "    \"\"\"\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"user_query\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# user_query = \"agent memory\"\n",
    "user_query = \"I like an apple\"\n",
    "\n",
    "result = rag_chain.invoke(user_query)\n",
    "\n",
    "print(result)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
